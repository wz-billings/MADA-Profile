<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Tidy Tuesday</title>

<script src="site_libs/header-attrs-2.11/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "Óâô";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "Óâô";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">My Data Analysis Portfolio</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="./aboutme.html">About Me</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Projects
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="./rcoding.html">R Coding</a>
    </li>
    <li>
      <a href="./visualization.html">Visualization</a>
    </li>
    <li>
      <a href="./flu_models.html">ML Models</a>
    </li>
    <li>
      <a href="./tidytuesday.html">Tidy Tuesday</a>
    </li>
    <li>
      <a href="./tidytuesday2.html">Tidy Tuesday 2</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/wz-billings/ZaneBillings-MADA-portfolio">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Tidy Tuesday</h1>

</div>


<div id="nber-papers-data" class="section level1" number="1">
<h1><span class="header-section-number">1</span> NBER papers data</h1>
<p>This week‚Äôs Tidy Tuesday dataset is a dataset of non-peer-reviewed economic papers produced by the National Bureau of Economic Research (NBER). First things first, we need to load the data.</p>
<p>At the time of writing, neither the CRAN nor GitHub versions of the <code>tidytuesdayR</code> package have been updated with the new data, so instead I will just get the data from the <code>nberwp</code> package. I‚Äôve also chosen to load all necessary packages here.</p>
<pre class="r"><code>library(nberwp) # contains data
library(emo) # for emojis
library(tidyverse) # for data cleaning, wrangling, and plotting
library(lubridate) # for datetimes
library(cowplot) # for theme_cowplot
library(fastDummies) # for creating dummy variables from factors
library(tidymodels) # for predictive modeling
library(tidylo) # for log odds
library(tidytext) # for feature engineering from unstructured text
library(textrecipes) # for feature engineering from unstructured text
library(stopwords) # needed for step_stopwords
library(finetune) # for my favorite tuning methods
library(lme4) # need for finetune
library(glmnet) # for the model I want to fit
library(colorblindr) # for better plot colors

ggplot2::theme_set(cowplot::theme_cowplot())</code></pre>
</div>
<div id="data-cleaning-wrangling-and-exploration" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Data cleaning, wrangling, and exploration</h1>
<p>Now we can join all of the individual datasets together. The code for this was partially provided on the TidyTuesday GitHub page. In this section I will also do a little data cleaning to make sure variables are in the correct format for analysis. I‚Äôve also filtered out any records that are missing the program category.</p>
<pre class="r"><code>dat &lt;- dplyr::left_join(papers, paper_authors, by = &quot;paper&quot;) %&gt;% 
  dplyr::left_join(authors, by = &quot;author&quot;) %&gt;% 
  dplyr::left_join(paper_programs, by = &quot;paper&quot;) %&gt;% 
  dplyr::left_join(programs, by = &quot;program&quot;) %&gt;%
  tidyr::drop_na(program_category) %&gt;%
  dplyr::mutate(
    catalogue_group = stringr::str_sub(paper, 1, 1),
    catalogue_group = factor(
      dplyr::case_when(
        catalogue_group == &quot;h&quot; ~ &quot;Historical&quot;,
        catalogue_group == &quot;t&quot; ~ &quot;Technical&quot;,
        catalogue_group == &quot;w&quot; ~ &quot;General&quot;
      )
    ),
    program_category = dplyr::case_when(
      program_category == &quot;Macro/International&quot; ~ &quot;Macro&quot;,
      TRUE ~ program_category
    ),
    paper_date = lubridate::ymd(paste0(year, &quot;-&quot;, month, &quot;-01&quot;)),
    dplyr::across(dplyr::contains(&quot;program&quot;), as.factor),
    .after = paper
  ) %&gt;%
  # Let&#39;s get rid of the variables that contain redundant information.
  dplyr::select(-year, -month, -user_nber, -user_repec, -program, -author)

dplyr::glimpse(dat)</code></pre>
<pre><code>## Rows: 128,565
## Columns: 7
## $ paper            &lt;chr&gt; &quot;w0074&quot;, &quot;w0087&quot;, &quot;w0087&quot;, &quot;w0087&quot;, &quot;w0087&quot;, &quot;w0107&quot;, &quot;w0116&quot;, &quot;w0117&quot;, &quot;w012~
## $ catalogue_group  &lt;fct&gt; General, General, General, General, General, General, General, General, Gener~
## $ paper_date       &lt;date&gt; 1975-03-01, 1975-05-01, 1975-05-01, 1975-05-01, 1975-05-01, 1975-10-01, 1975~
## $ title            &lt;chr&gt; &quot;Variation Across Household in the Rate of Inflation&quot;, &quot;Exports and Foreign I~
## $ name             &lt;chr&gt; &quot;Robert T Michael&quot;, &quot;Merle Yahr Weiss&quot;, &quot;Merle Yahr Weiss&quot;, &quot;Robert E Lipsey&quot;~
## $ program_desc     &lt;fct&gt; &quot;Economic Fluctuations and Growth&quot;, &quot;International Finance and Macroeconomics~
## $ program_category &lt;fct&gt; Macro, Macro, Macro, Macro, Macro, Micro, Micro, Micro, Micro, Macro, Macro, ~</code></pre>
<p>We can see that the data has 128565 records while there are only 28952 paper IDs in the dataset, so each paper is currently represented by multiple records. I suspect that this occurs when papers have either multiple authors or associated programs.</p>
<pre class="r"><code>dat %&gt;%
  dplyr::distinct(paper, name) %&gt;%
  dplyr::count(paper) %&gt;%
  ggplot(aes(x = n)) +
  geom_bar(col = &quot;black&quot;, fill = &quot;gray&quot;) +
  geom_label(aes(y = after_stat(count), label = after_stat(count)),
             stat = &quot;count&quot;, nudge_y = 500) +
  scale_x_continuous(breaks = seq(1:17), expand = c(0, 0.1)) +
  labs(
    x = &quot;number of authors on a paper&quot;,
    y = &quot;frequency&quot;
  )</code></pre>
<div class="figure">
<img src="tidytuesday_files/figure-html/number%20of%20authors%20per%20paper-1.png" alt="A column chart with number of authors of a paper on the horizontal axis, and the number of papers with that many authors in the dataset on the y-axis." width="672" />
<p class="caption">
(#fig:number of authors per paper)Most papers have 4 or less authors, however some have up to 17.
</p>
</div>
<pre class="r"><code>dat %&gt;%
  dplyr::distinct(paper, program_desc) %&gt;%
  dplyr::count(paper) %&gt;%
  ggplot(aes(x = n)) +
  geom_bar(col = &quot;black&quot;, fill = &quot;gray&quot;) +
  geom_label(aes(y = after_stat(count), label = after_stat(count)),
             stat = &quot;count&quot;, nudge_y = 500) +
  scale_x_continuous(breaks = seq(1:14), expand = c(0, 0.1)) +
  labs(
    x = &quot;number of programs associated with a paper&quot;,
    y = &quot;frequency&quot;
  )</code></pre>
<div class="figure">
<img src="tidytuesday_files/figure-html/number%20of%20programs%20per%20paper-1.png" alt="A column chart with number of programs associated with a on the horizontal axis, and the number of papers with that many associated programs in the dataset on the y-axis." width="672" />
<p class="caption">
(#fig:number of programs per paper)Most papers have 4 or less associated programs, however some have up to 14.
</p>
</div>
<p>It appears that this assumption is likely correctly. Since the author is unstructured text data that will likely not be very useful to us, and the programs would probably be better structured as a dummy variable, we will reformat this data into a wide form where author and program are one column with comma-separated values. This format should be relatively easy to use for feature engineering later.</p>
<p>I‚Äôll leave the program descriptions and title alone for now‚ÄìI think we can transform these into more informative text data. I don‚Äôt think the authors will be very useful, but maybe counting them could be so I‚Äôll include a feature for that.</p>
<pre class="r"><code>collapse_to_str &lt;- function(.x, delim = &quot;, &quot;) {
  paste0(sort(unique(.x)), collapse = delim)
}

dat_wide &lt;- dat %&gt;%
  dplyr::group_by(paper) %&gt;%
  dplyr::summarize(
    authors = collapse_to_str(name),
    num_authors = length(unique(name)),
    program_cats = collapse_to_str(program_category),
    programs = collapse_to_str(program_desc),
    .groups = &quot;drop&quot;
  )

LUT &lt;- dat %&gt;%
  dplyr::select(paper, title, paper_date) %&gt;%
  dplyr::distinct()

dat2 &lt;- dat_wide %&gt;%
  dplyr::left_join(LUT, by = &quot;paper&quot;)

dplyr::glimpse(dat2)</code></pre>
<pre><code>## Rows: 28,952
## Columns: 7
## $ paper        &lt;chr&gt; &quot;h0001&quot;, &quot;h0002&quot;, &quot;h0003&quot;, &quot;h0004&quot;, &quot;h0005&quot;, &quot;h0006&quot;, &quot;h0007&quot;, &quot;h0008&quot;, &quot;h0009&quot;, ~
## $ authors      &lt;chr&gt; &quot;Robert William Fogel&quot;, &quot;Michael R Haines&quot;, &quot;Richard C Sutch, Roger L Ransom&quot;, &quot;C~
## $ num_authors  &lt;int&gt; 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 3, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1, ~
## $ program_cats &lt;chr&gt; &quot;Micro&quot;, &quot;Micro&quot;, &quot;Micro&quot;, &quot;Micro&quot;, &quot;Micro&quot;, &quot;Micro&quot;, &quot;Micro&quot;, &quot;Micro&quot;, &quot;Micro&quot;, ~
## $ programs     &lt;chr&gt; &quot;Development of the American Economy&quot;, &quot;Development of the American Economy&quot;, &quot;De~
## $ title        &lt;chr&gt; &quot;Second Thoughts on the European Escape from Hunger: Famines, Price Elasticities,~
## $ paper_date   &lt;date&gt; 1989-05-01, 1989-05-01, 1989-05-01, 1989-05-01, 1989-08-01, 1989-08-01, 1989-10-~</code></pre>
<p>After looking at the data, I think that there are a few interesting questions we could ask using topic modeling, which is what I really wanted to do when I saw this dataset. My heart‚Äôs desire is to try and predict which categories a paper will have based on information like the title, date, number of authors, etc. (Not sure what other predictors might be derived from this dataset.) Note that each paper can have more than one category. We‚Äôll address this issue in a bit.</p>
</div>
<div id="data-exploration" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Data exploration</h1>
<p>We have a few potential predictors of interest:
* Date published (month and year are potentially both predictors of category);
* Number of authors a paper has; and
* Any information which can be gleaned from the title of the paper.</p>
<p>I do not plan to use any information about the authors (I suspect certain authors are strongly linked to which categories a paper has, but I just don‚Äôt think this is very interesting, and would also perform poorly out-of-sample) or the programs (which are subdivisions of category). Let‚Äôs try to explore each of these four potential predictors. In order to explore the effects on each category separately, we need to do one more data cleaning step: turning each of the categories into a dummy variable.</p>
<pre class="r"><code>dat3 &lt;- dat2 %&gt;%
  fastDummies::dummy_cols(
    select_columns = c(&quot;program_cats&quot;),
    split = &quot;,&quot;,
    remove_selected_columns = TRUE
  )

dplyr::glimpse(dat3)</code></pre>
<pre><code>## Rows: 28,952
## Columns: 9
## $ paper                &lt;chr&gt; &quot;h0001&quot;, &quot;h0002&quot;, &quot;h0003&quot;, &quot;h0004&quot;, &quot;h0005&quot;, &quot;h0006&quot;, &quot;h0007&quot;, &quot;h0008&quot;, &quot;~
## $ authors              &lt;chr&gt; &quot;Robert William Fogel&quot;, &quot;Michael R Haines&quot;, &quot;Richard C Sutch, Roger L Ran~
## $ num_authors          &lt;int&gt; 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 3, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2~
## $ programs             &lt;chr&gt; &quot;Development of the American Economy&quot;, &quot;Development of the American Econo~
## $ title                &lt;chr&gt; &quot;Second Thoughts on the European Escape from Hunger: Famines, Price Elast~
## $ paper_date           &lt;date&gt; 1989-05-01, 1989-05-01, 1989-05-01, 1989-05-01, 1989-08-01, 1989-08-01, ~
## $ program_cats_Finance &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ program_cats_Macro   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ program_cats_Micro   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~</code></pre>
<p>Having a version of this data in long form will be helpful as well.</p>
<pre class="r"><code>dat3_long &lt;- dat3 %&gt;%
  pivot_longer(
    cols = starts_with(&quot;program_cats&quot;),
    names_pattern = &quot;program_cats_(.*)&quot;
  ) %&gt;%
  dplyr::filter(value != 0)</code></pre>
<p>First, let‚Äôs look at the overall effect of date, and then decompose this trend into year and month.</p>
<pre class="r"><code>dat3_long %&gt;%
  dplyr::group_by(name, paper_date) %&gt;%
  dplyr::count() %&gt;%
  dplyr::ungroup() %&gt;%
  ggplot(aes(x = paper_date, y = n, color = name, fill = name)) +
  geom_line(alpha = 0.6) +
  geom_smooth(method = &quot;loess&quot;) +
  labs(
    x = &quot;calendar date&quot;,
    y = &quot;number of papers released in month&quot;,
    color = &quot;paper category&quot;,
    fill = &quot;paper category&quot;
  ) +
  theme(legend.position = c(0.1, 0.7)) +
  coord_cartesian(expand = FALSE)</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<div class="figure">
<img src="tidytuesday_files/figure-html/date%20counts-1.png" alt="A line chart showing the number of papers released in a given month on the y-axis and the calendar date, from 1980 to 2020, on the x-axis. There is one line for each of the three paper categories. There is a smoothed LOESS curve for each group." width="672" />
<p class="caption">
(#fig:date counts)The three groups are quite similar in early years, but the counts diverge for the groups over time. The bold curve is the LOESS trend.
</p>
</div>
<p>Next let‚Äôs look at the cumulative count.</p>
<pre class="r"><code>dat3_long %&gt;%
  dplyr::group_by(name, paper_date) %&gt;%
  dplyr::count() %&gt;%
  dplyr::ungroup(paper_date) %&gt;%
  dplyr::arrange(paper_date) %&gt;%
  dplyr::mutate(nn = cumsum(n)) %&gt;%
  dplyr::ungroup() %&gt;%
  ggplot(aes(x = paper_date, y = nn, color = name)) +
  geom_line(size = 1) +
  labs(
    x = &quot;calendar date&quot;,
    y = &quot;cumulative number of papers released up to date&quot;,
    color = &quot;paper category&quot;
  ) +
  theme(legend.position = c(0.1, 0.7)) +
  colorblindr::scale_color_OkabeIto() +
  coord_cartesian(expand = FALSE)</code></pre>
<div class="figure">
<img src="tidytuesday_files/figure-html/date%20cumulative%20counts-1.png" alt="A line chart of cumulative number of papers released up to date vs. the calendar date. There is a separate line for each of the three paper categories." width="672" />
<p class="caption">
(#fig:date cumulative counts)The cumulative counts over time show a similar trend to the overall counts.
</p>
</div>
<p>If we use a cumulative count over the month, we can look at the effect of year, smoothed to ignore any trend from the month.</p>
<pre class="r"><code>dat3_long %&gt;%
  dplyr::group_by(name, yr = lubridate::year(paper_date)) %&gt;%
  dplyr::count(name = &quot;individual count&quot;) %&gt;%
  dplyr::ungroup(yr) %&gt;%
  dplyr::arrange(name, yr) %&gt;%
  dplyr::mutate(&quot;cumulative count&quot; = cumsum(`individual count`)) %&gt;%
  dplyr::ungroup() %&gt;%
  tidyr::pivot_longer(
    cols = ends_with(&quot;count&quot;),
    names_to = &quot;how&quot;, values_to = &quot;count&quot;
  ) %&gt;%
  ggplot(aes(x = yr, y = count, color = name)) +
  geom_line(size = 1) +
  facet_wrap(vars(how), scales = &quot;free_y&quot;) +
  labs(
    x = &quot;calendar year&quot;,
    y = NULL,
    color = &quot;paper category&quot;
  ) +
  theme(legend.position = &quot;bottom&quot;, legend.justification = &quot;center&quot;) +
  colorblindr::scale_color_OkabeIto()</code></pre>
<div class="figure">
<img src="tidytuesday_files/figure-html/year%20counts-1.png" alt="A line plot with two facets. The left facet shows cumulative count of papers vs. calendar year, smoothed over months, and the right facet shows the count of papers published each month vs. calendar year. Both plots have one line for each paper category." width="672" />
<p class="caption">
(#fig:year counts)The trend for years shows exactly the same trend as the previous plots, indicating that the year contributes mainly to the overall trend. This is expected and unsurprising.
</p>
</div>
<p>Finally, we can directly examine whether month has an effect or not.</p>
<pre class="r"><code>dat3_long %&gt;%
  dplyr::group_by(name,
                  mth = lubridate::month(paper_date, label = TRUE),
                  yr = lubridate::year(paper_date)) %&gt;%
  dplyr::count() %&gt;%
  dplyr::ungroup(yr) %&gt;%
  dplyr::summarize(mean_monthly = mean(n)) %&gt;%
  dplyr::mutate(mean_category = mean(mean_monthly)) %&gt;%
  dplyr::ungroup() %&gt;%
  ggplot(aes(y = forcats::fct_rev(mth), x = mean_monthly)) +
  geom_col(color = &quot;black&quot;, fill = &quot;gray&quot;) +
  facet_wrap(vars(name), scales = &quot;free_x&quot;) +
  geom_vline(aes(xintercept = mean_category, group = name), size = 1, lty = 2) +
  labs(x = &quot;mean number of papers released&quot;, y = &quot;month&quot;)</code></pre>
<pre><code>## `summarise()` has grouped output by &#39;name&#39;. You can override using the `.groups` argument.</code></pre>
<div class="figure">
<img src="tidytuesday_files/figure-html/month%20counts-1.png" alt="A horizontal bar chart with three facets. The first facet (starting from the right) is for the finance category, the middle is for the macro category, and the left is for the micro category. All three charts show the mean number of papers published on the x-axis and the month on the y-axis. Each facet has a dashed line indicating the overall mean for each category." width="672" />
<p class="caption">
(#fig:month counts)The dashed line in this plot shows the mean across all months. We see that across all three categories, there are really no noticeable deviations from this mean, except for maybe in July but this trend is so small it is not worth investigating. There appears to be no seasonality by month.
</p>
</div>
<p>Now let‚Äôs look at the effect of the number of authors.</p>
<pre class="r"><code>dat3_long %&gt;%
  dplyr::group_by(num_authors, name) %&gt;%
  dplyr::count() %&gt;%
  ggplot(aes(x = as.factor(num_authors), y = n)) +
  geom_col(color = &quot;black&quot;, fill = &quot;gray&quot;) +
  facet_wrap(vars(name), ncol = 1, scales = &quot;free_y&quot;) +
  labs(x = &quot;number of authors&quot;, y = &quot;frequency&quot;)</code></pre>
<div class="figure">
<img src="tidytuesday_files/figure-html/author%20count%20effect-1.png" alt="A bar chart with three facets. The top is for the finance category, the middle is for the macro category, and the bottom is for the micro category. Each facet shows the number of authors on the x-axis, and the count of how many papers in the facet category have that number of authors on the y-axis." width="672" />
<p class="caption">
(#fig:author count effect)The distribution of number of papers with a given number of authors appears to be similarly shaped across all three paper categories and is likely not predictive.
</p>
</div>
<p>Now we can take a look at the unstructured text data. This can be a bit complicated, but there are a few good visualizations. I am going to look at the effect of individual words only‚Äì<em>n</em>-grams could also potentially be predictive, but are more complicated. If I were being paid for this, I would probably at least look at bigrams, but I am not being paid for this.</p>
<p>A quick note on stop words: sometimes, it is true that stop words can be predictive of certain categories. However, I simply do not think that these are very interesting predictors, so I am going to take them out. First let‚Äôs process the text data.</p>
<pre class="r"><code>dat3_tp &lt;- dat3_long %&gt;%
  tidytext::unnest_tokens(word, title) %&gt;%
  dplyr::anti_join(tidytext::stop_words, by = &quot;word&quot;) %&gt;%
  # Remove words that are numbers
  dplyr::filter(stringr::str_detect(word, &quot;[0-9]&quot;, negate = TRUE))</code></pre>
<p>Now let‚Äôs look at which words occur most frequently for each category.</p>
<pre class="r"><code>dat3_tp %&gt;%
  group_by(name) %&gt;%
  count(word) %&gt;%
  slice_max(order_by = n, n = 10) %&gt;%
  ggplot(aes(x = n, y = tidytext::reorder_within(word, n, name))) +
  geom_col(color = &quot;black&quot;, fill = &quot;gray&quot;) +
  facet_wrap(vars(name), scales = &quot;free&quot;) +
  scale_y_reordered() +
  labs(x = &quot;frequency&quot;, y = NULL)</code></pre>
<div class="figure">
<img src="tidytuesday_files/figure-html/word%20counts-1.png" alt="A bar chart with three facets, one for each of the three paper categories. Each bar chart shows the top ten most frequently occurring words in the titles of each category of paper." width="672" />
<p class="caption">
(#fig:word counts)While some words overlap between categories, such as evidence, there are definitely differential word counts among the three groups. Since there are high-count words which overlap, we need to use a different measure to determine what title words are predictive of category.
</p>
</div>
<p>For our final exploratory plot, let‚Äôs make a more complicated visualization: we can use the <code>tidylo</code> package to compute the log-odds of a word being associated with a particular category. There are a few pitfalls here (we will not be taking correlations between categories into account, notably), but for the sake of statistical simplicity I am going to ignore that. This can potentially be a lot more useful than raw counts for determining if a particular word will be predictive for a category.</p>
<pre class="r"><code>dat3_tp %&gt;%
  group_by(name) %&gt;%
  count(word) %&gt;%
  ungroup() %&gt;%
  bind_log_odds(set = name, feature = word, n = n) %&gt;%
  group_by(name) %&gt;%
  mutate(lo = log_odds_weighted) %&gt;%
  arrange(lo) %&gt;%
  filter(row_number() &gt; max(row_number()) - 5 | row_number() &lt;= 5) %&gt;%
  ggplot(aes(x = lo, y = tidytext::reorder_within(word, lo, name))) +
  geom_vline(xintercept = 0) +
  geom_col(color = &quot;black&quot;, fill = &quot;gray&quot;) +
  facet_wrap(vars(name), scales = &quot;free_y&quot;) +
  scale_y_reordered() +
  labs(x = &quot;weighted log-odds&quot;, y = NULL, fill = NULL) +
  theme(legend.position = &quot;bottom&quot;, legend.justification = &quot;center&quot;)</code></pre>
<div class="figure">
<img src="tidytuesday_files/figure-html/word%20log%20odds-1.png" alt="A bar chart with three facets, one for each paper category. Each chart shows the five words with the highest and five words with the lowest weighted log-odds for that category." width="672" />
<p class="caption">
(#fig:word log odds)Whereas the counts made it quite difficult to distinguish between categories due to overlap, we can see that there are unique words associated with each category that have large predictive power. For macro and micro categories, there are words that are both highly predictive and highly anti-predictive, while for finance there are words which are highly predictive, but not many words which are highly anti-predictive. The top 5 maximum and bottom 5 minimum log-odds words are shown for each category.
</p>
</div>
<p>OK, great. Now that we have explored our data a bit, we need to decide how we will actually model the categories.</p>
</div>
<div id="modeling-strategy" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Modeling strategy</h1>
<p>Since each paper has multiple categories, it‚Äôs difficult to categorize papers to categories (in general a multi-label classification problem is much more difficult than a single-label classification problem). There are a few strategies we can take here:</p>
<ol style="list-style-type: decimal">
<li><p>Reduce the question to a simpler question of interest. E.g., can we predict whether a given paper will have the ‚Äúmicro‚Äù category or not? This is probably the easiest solution but it is the least exciting.</p></li>
<li><p>Using the <strong>label power set</strong> (LP) transformation: treat the outcome as a factor with sparse levels, where the possible results are the power set of the set of labels (so e.g.¬†‚ÄúMicro, macro, finance‚Äù would be a separate level, as would ‚Äúmacro‚Äù only, and ‚Äúmacro and finance‚Äù as well as all combinations).</p></li>
<li><p>The <strong>binary relevance</strong> method: train a model to predict each category independently, and then combine the results of these models using some kind of voting/discrimination scheme. (The simplest voting scheme is to train one model for each category, and say yes for a category in the multi-label prediction if the related model says yes.) There are more complex versions of this as well, such as the classifier chain method.</p></li>
<li><p>An <strong>ensemble</strong> method: train multiple standard multiclass learners which all predict a single class, and then use a voting scheme to select multiple categories.</p></li>
<li><p>Use a more complicated algorithm specifically for multi-label classification. I don‚Äôt really want to do this because I don‚Äôt understand any of them that well. Maybe it is the best option but I can‚Äôt use <code>tidymodels</code> for this.</p></li>
</ol>
<p>The LP method is probably the ‚Äúeasiest‚Äù in terms of just using models and code that already exist. But this can potentially be quite annoying if any of the levels are sparse, so let‚Äôs take a quick look at that.</p>
<pre class="r"><code>dat2 %&gt;%
  count(program_cats) %&gt;%
  mutate(program_cats = forcats::fct_reorder(program_cats, n)) %&gt;%
  ggplot(aes(x = program_cats, y = n)) +
  geom_col(col = &quot;black&quot;, fill = &quot;gray&quot;) +
  geom_label(aes(label = n), nudge_y = 700) +
  labs(
    x = &quot;set of paper categories&quot;,
    y = &quot;frequency&quot;
  ) +
  coord_flip()</code></pre>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="tidytuesday_files/figure-html/unnamed-chunk-1-1.png" alt="A horizontal bar chart showing the levels of the power set of paper categories on the y-axis, and the count of papers which have those categories on the x-axis. Each bar has a text label showing the number of papers." width="672" />
<p class="caption">
Figure 4.1: Counts of outcome categories after using the level powerset transformation on the set of potential labels. There is a large amount of class imbalance, which suggests that the LP method with a standard multiclass learner may be an inefficient approach to this problem.
</p>
</div>
<p>The classes are severely imbalanced when the LP transformation is implied‚Äìthe distribution of counts implies that micro only will be the easiest label to predict, but we can even see that the simpler problem of classifying papers into having vs.¬†not having the micro category would also pose a class imbalance problem. Next let‚Äôs examine whether the binary relevance method would have a similar class imbalance issue. We can construct dummy variables for each of the three categories and examine the frequency of each category overall.</p>
<pre class="r"><code>dat3 %&gt;%
  pivot_longer(
    cols = starts_with(&quot;program_cats&quot;),
    names_pattern = &quot;program_cats_(.*)&quot;
  ) %&gt;%
  group_by(name) %&gt;%
  summarize(val = sum(value)) %&gt;%
  ggplot(aes(y = forcats::fct_reorder(name, val), x = val)) +
  geom_vline(xintercept = nrow(dat2)/2, size = 2, lty = 2) + 
  geom_col(col = &quot;black&quot;, fill = &quot;gray&quot;) +
  geom_label(aes(label = paste0(val, &quot;\n &quot;, round(val/nrow(dat2) * 100), &quot;%&quot;)),
             nudge_x = 1400, size = 5) +
  geom_vline(xintercept = nrow(dat2), size = 2, lty = 2) +
  annotate(&quot;label&quot;, x = nrow(dat2) - 1000, y = &quot;Macro&quot;,
           label = paste(&quot;total:&quot;, nrow(dat2)), size = 5) +
  labs(
    x = &quot;frequency&quot;,
    y = &quot;program category&quot;
  )</code></pre>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-2"></span>
<img src="tidytuesday_files/figure-html/unnamed-chunk-2-1.png" alt="A horizontal bar chart showing the program category on the y-axis and the total number of papers which have that category associated on the x-axis. There is a vertical line showing the total number of papers (28952) as well, and each bar is labeled with the count and percentage of papers that fall into that category." width="672" />
<p class="caption">
Figure 4.2: Distribution of classes for each category. The closer a bar is to the middle, the closer that class is to being balanced being positive and negative occurrences in the observed data.
</p>
</div>
<p>We see that the micro and macro classes are both fairly balanced, but the finance class occurs less frequently and could potentially present issues in classification. I think a detailed treatment of upsampling and downsampling is beyond what I am inclined to do for the purpose of this model‚Äìthe good news is that when you‚Äôre building a predictive model you don‚Äôt have to use uniform preprocessing across all your ensemble candidate models because there are no rules except optimize your predictive metrics üòÅ but I think I‚Äôll just not worry about it since the only consequence of poor prediction metrics in this case is the damage to my dignity.</p>
<p>So, I have decided on the <strong>binary relevance</strong> technique for multi-label classification. I will train an independent model to predict presence or absence each of the three categories, and then these predictions can be combined. The downside of this method is that (especially when we are not using chained classifiers) we cannot use correlations between labels to our advantage. However, I think it will be OK to do here just as a fun exercise.</p>
<p>First things first, I always like to specify which prediction metric I‚Äôll focus on optimizing up-front. I think ideally we would want to tune all three of the independent models such that they optimize, e.g., the Hamming loss of the multi-label problem. But I am feeling a bit too lazy to code that, so I‚Äôll train each model on the same resamples and optimize for my personal favorite measure of binary classification accuracy, the MCC. Optimizing the three independent models does not necessarily equate to optimizing the multi-label model, but it will be good enough for me.</p>
<p>The final ingredient that we need to decide on is the modeling strategy for each of the three independent models. We could go crazy and fit an ensemble for each of the three models, but I think that‚Äôs overkill. My favorite model is the elastic net, which fortunately tends to work well with text-based predictors, so I‚Äôll build an elastic net model for each category.</p>
</div>
<div id="model-fitting" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Model fitting</h1>
<p>The first step is to <strong>plan our data budget</strong> (tidymodels dev Julia Silge uses this phrase and I really like it). We will split our data into training and testing sets, and then from the training set we will construct resamples using Monte Carlo cross validation, which I think is a great compromise between the advantages of LOO cross validation and how computationally LOO cv is to actually implement on a large dataset. For this exercise I‚Äôll only use 100 resamples so this doesn‚Äôt take years for the model to run. When I learned this the first time, I learned that 70% training is a good balance so that‚Äôs what I always use, although I‚Äôm sure whatever is probably fine.</p>
<pre class="r"><code>set.seed(370)
dat_split &lt;- dat3 %&gt;% rsample::initial_split(prop = 0.7)
dat_train &lt;- rsample::training(dat_split)
dat_test &lt;- rsample::testing(dat_split)
dat_resamples &lt;- dat_train %&gt;% rsample::mc_cv(times = 20)</code></pre>
<p>Now we‚Äôll need to do a little bit of preprocessing. We can do this using the <code>recipes</code> package. (Note that the text is not yet processed in this dataframe because I did my work in a weird order.) We actually need to set up three recipes in this case, one for each outcome variable. Normally I would like to tune the number of tokens to include in the model, but in this case I‚Äôll just choose the top 100 to reduce tuning time. If that is too many, they should get eliminated by the model anyways.</p>
<pre class="r"><code>micro_recipe &lt;- dat3 %&gt;%
  # Recipe with the formula specification for the model
  recipe(program_cats_Micro ~ paper_date + title) %&gt;%
  # Make sure outcome is a factor that glmnet won&#39;t coerce to numeric
  step_bin2factor(all_outcomes()) %&gt;%
  # Turn the title into individual words
  step_tokenize(title) %&gt;%
  # Remove stopwords
  step_stopwords(title) %&gt;%
  # We don&#39;t need every word, but we aren&#39;t sure how many we need.
  step_tokenfilter(title, max_tokens = 100) %&gt;%
  # Turn tokenlist into tf/idf
  step_tfidf(title) %&gt;%
  # extract year from date
  step_date(paper_date, features = &quot;year&quot;, keep_original_cols = FALSE)

macro_recipe &lt;- dat3 %&gt;%
  recipe(program_cats_Macro ~ paper_date + title) %&gt;%
  step_bin2factor(all_outcomes()) %&gt;%
  step_tokenize(title) %&gt;%
  step_stopwords(title) %&gt;%
  step_tokenfilter(title, max_tokens = 100) %&gt;%
  step_tfidf(title) %&gt;%
  step_date(paper_date, features = &quot;year&quot;, keep_original_cols = FALSE)

finance_recipe &lt;- dat3 %&gt;%
  recipe(program_cats_Finance ~ paper_date + title) %&gt;%
  step_bin2factor(all_outcomes()) %&gt;%
  step_tokenize(title) %&gt;%
  step_stopwords(title) %&gt;%
  step_tokenfilter(title, max_tokens = 100) %&gt;%
  step_tfidf(title) %&gt;%
  step_date(paper_date, features = &quot;year&quot;, keep_original_cols = FALSE)</code></pre>
<p>So now our preprocessors are set up. Next we can set up our elastic net model, using the <code>glmnet</code> engine. We will tune both the penalty and the mixture parameters of the model.</p>
<pre class="r"><code># We want to use logistic regression
model_spec &lt;- parsnip::logistic_reg(penalty = tune(), mixture = tune()) %&gt;%
  # use the glmnet package to fit the model
  set_engine(&quot;glmnet&quot;) %&gt;%
  # we are doing a classification problem
  set_mode(&quot;classification&quot;)</code></pre>
<p>Now we can define a workflow set, which will bundle this model together with each of our preprocessors.</p>
<pre class="r"><code>wf_set &lt;- workflow_set(
  preproc = list(micro = micro_recipe,
                 macro = macro_recipe,
                 finance = finance_recipe),
  model = list(mod = model_spec)
)</code></pre>
<p>Now we can tune each model in the workflow set. My favorite tuning/optimization algorithm is simulated annealing, but I think that‚Äôs a bit overkill here. However, instead of doing something like a grid search, I‚Äôll speed up our results a bit by using a racing method from the <code>finetune</code> package. Normally we would want to use a lot more resamples and a lot more parameter options here, but I do not want this page to take 10 hours to build.</p>
<pre class="r"><code># Register a parallel backend for improved speed. This interacts
# with the improvement gained by using a racing method to improve
# computation time even more, if there is no significant overhead.
doParallel::registerDoParallel()

tuned &lt;- wf_set %&gt;%
  workflow_map(
    fn = &quot;tune_race_anova&quot;,
    resamples = dat_resamples,
    grid = 10,
    metrics = metric_set(mcc),
    verbose = TRUE 
  )</code></pre>
<pre><code>## i 1 of 3 tuning:     micro_mod</code></pre>
<pre><code>## v 1 of 3 tuning:     micro_mod (3m 20s)</code></pre>
<pre><code>## i 2 of 3 tuning:     macro_mod</code></pre>
<pre><code>## v 2 of 3 tuning:     macro_mod (2m 10.7s)</code></pre>
<pre><code>## i 3 of 3 tuning:     finance_mod</code></pre>
<pre><code>## v 3 of 3 tuning:     finance_mod (2m 53.4s)</code></pre>
<pre class="r"><code># It&#39;s always polite to close your cluster when you finish
doParallel::stopImplicitCluster()</code></pre>
<p>Now that the models are tuned, we need to extract them. Unfortunately there isn‚Äôt really a slick way to do this üôÑ, hopefully the tidymodels team will expand <code>workflow_map</code> capabilities in the future to make this easier. I also think I am using it differently from how the creators intended here, but that has never stopped me before.</p>
<p>Since we didn‚Äôt do a regular grid search (and the search we used was not very dense anyways), we can‚Äôt make a nice heatmap, of MCC for each group by the two hyperparameters, so I‚Äôll settle for a table.</p>
<pre class="r"><code>tuned %&gt;%
  dplyr::mutate(metrics = map(result, collect_metrics)) %&gt;%
  dplyr::select(wflow_id, metrics) %&gt;%
  tidyr::unnest(cols = metrics) %&gt;%
  dplyr::group_by(wflow_id) %&gt;%
  dplyr::slice_max(mean, n = 1) %&gt;%
  dplyr::ungroup() %&gt;%
  dplyr::transmute(
    category = gsub(&quot;_mod&quot;, &quot;&quot;, wflow_id),
    `log10(penalty)` = scales::number(log10(penalty), accuracy = 0.0001),
    mixture = scales::number(mixture, accuracy = 0.0001),
    `mean MCC` = scales::number(mean, accuracy = 0.0001),
    `std. err.` = scales::number(std_err, accuracy = 0.0001)
  ) %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr>
<th style="text-align:left;">
category
</th>
<th style="text-align:left;">
log10(penalty)
</th>
<th style="text-align:left;">
mixture
</th>
<th style="text-align:left;">
mean MCC
</th>
<th style="text-align:left;">
std. err.
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
finance
</td>
<td style="text-align:left;">
-8.5926
</td>
<td style="text-align:left;">
0.1689
</td>
<td style="text-align:left;">
0.3284
</td>
<td style="text-align:left;">
0.0032
</td>
</tr>
<tr>
<td style="text-align:left;">
macro
</td>
<td style="text-align:left;">
-2.7274
</td>
<td style="text-align:left;">
0.8868
</td>
<td style="text-align:left;">
0.4472
</td>
<td style="text-align:left;">
0.0024
</td>
</tr>
<tr>
<td style="text-align:left;">
micro
</td>
<td style="text-align:left;">
-5.0193
</td>
<td style="text-align:left;">
0.4265
</td>
<td style="text-align:left;">
0.4755
</td>
<td style="text-align:left;">
0.0031
</td>
</tr>
</tbody>
</table>
<p>Interestingly, it looks like all of the models are quite different‚Äìfor the micro category, a model closer to LASSO is operator, while for macro and finance, a model closer to ridge regression is better. Now let‚Äôs apply the best predictions to our model specifications, and then we can fit the model.</p>
<p>‚Ä¶or, that would be my next goal anyways. Now I‚Äôve hit my time limit on this assignment, so I decided to stop here üòÅ. Maybe if I ever have some spare time (yeah right), I‚Äôll come back and evaluate the models, but for now this is how they will remain.</p>
<!--



# Model results
-->
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
